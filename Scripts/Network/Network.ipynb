{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.python.keras.models import Input, Model\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPool2D, concatenate, Dense, Conv3D, MaxPool3D\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "BSE_SHAPE = (500, 500, 1)\n",
    "EBSD_SHAPE = (200, 200, 3)\n",
    "OM_SHAPE = (1000, 1000, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "CONV_NODES = [64, 128, 256, 512]\n",
    "DENSE_NODES = [32, 64, 128]\n",
    "\n",
    "NUM_CONV = [0, 1]\n",
    "NUM_DENSE = [0, 1, 2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# TRAIN DATA IMPORT\n",
    "BSE = 'path'\n",
    "EBSD = 'path'\n",
    "OM = 'path'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# # #### testing network ####\n",
    "# test_path = 'test_data/'\n",
    "# \n",
    "# # load test img\n",
    "# i = load_img('test_data/0/bse.png')\n",
    "# x = img_to_array(i)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# #### Build Model ####\n",
    "# INPUTS\n",
    "input_1 = Input(shape=(1000,1000,1))\n",
    "input_2 = Input(shape=(100,100,3))\n",
    "input_3 = Input(shape=(2000,2000,1))\n",
    "\n",
    "# MODEL\n",
    "\n",
    "# FIRST BRANCH\n",
    "# BSE\n",
    "# needs to accept a black and white image\n",
    "one = Conv2D(64, (3, 3), activation=\"relu\")(input_1)\n",
    "one = MaxPool2D(pool_size=(2, 2))(one)\n",
    "\n",
    "one = Conv2D(64, 3, activation='relu')(one)\n",
    "one = MaxPool2D(pool_size=(2, 2), strides=2, padding='valid')(one)\n",
    "\n",
    "one = Model(inputs=input_1, outputs=one)\n",
    "\n",
    "# SECOND BRANCH\n",
    "# EBSD\n",
    "# needs to accept a RGB image\n",
    "two = Conv2D(64, (3, 3), activation='relu')(input_2)\n",
    "two = MaxPool2D(pool_size=(2, 2))(two)\n",
    "\n",
    "two = Conv2D(64, 3, activation='relu')(two)\n",
    "two = MaxPool2D(pool_size=(2, 2), strides=2, padding='valid')(two)\n",
    "\n",
    "two = Model(inputs=input_2, outputs=two)\n",
    "\n",
    "# THIRD BRANCH\n",
    "# OM\n",
    "# needs to accept a black and white image\n",
    "three = Conv2D(64, (3, 3), activation='relu')(input_3)\n",
    "three = MaxPool2D(pool_size=(2, 2))(three)\n",
    "\n",
    "three = Conv2D(64, 3, activation='relu')(three)\n",
    "three = MaxPool2D(pool_size=(2, 2), strides=2, padding='valid')(three)\n",
    "\n",
    "three = Model(inputs=input_3, outputs=three)\n",
    "\n",
    "# COMBINE THE BRANCHES\n",
    "# concatenates the three branches all together into one\n",
    "combined = concatenate([one.output, two.output])\n",
    "combined = concatenate([combined, three.output])\n",
    "\n",
    "# END BRANCH\n",
    "end = Dense(64, activation='relu')(combined)\n",
    "end = Dense(1, activation='sigmoid')(end)\n",
    "\n",
    "# MODEL\n",
    "model = Model(inputs=[one.input, two.input, three.input], outputs=end)\n",
    "\n",
    "# COMPILE MODEL\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# IMPORT DATA\n",
    "\n",
    "# load in data as an array of arrays which contain [bse, ebsd, om, stress]\n",
    "train_data = []\n",
    "for i in range(10):\n",
    "    path = 'test_data/SR/train/' + str(i) + '/'\n",
    "    bse = np.array(load_img(path + 'bse.png'))\n",
    "    bse = tf.keras.utils.normalize(bse, axis=1)\n",
    "    bse = bse.reshape(1_000, 1_000, 1)\n",
    "    \n",
    "    ebsd = np.array(load_img(path + 'ebsd.png'))\n",
    "    ebsd = tf.keras.utils.normalize(ebsd, axis=1)\n",
    "    ebsd = ebsd.reshape(100, 100, 3)\n",
    "    \n",
    "    om = np.array(load_img(path + 'om.png'))\n",
    "    om = tf.keras.utils.normalize(om, axis=1)\n",
    "    om = om.reshape(2000, 2000, 1)\n",
    "    \n",
    "    train_data.append([bse, ebsd, om, 1060])\n",
    "    \n",
    "for i in range(10):\n",
    "    path = 'test_data/HT/train/' + str(i) + '/'\n",
    "    bse = np.array(load_img(path + 'bse.png'))\n",
    "    bse = tf.keras.utils.normalize(bse, axis=1)\n",
    "    bse = bse.reshape(1_000, 1_000, 1)\n",
    "    \n",
    "    ebsd = np.array(load_img(path + 'ebsd.png'))\n",
    "    ebsd = tf.keras.utils.normalize(ebsd, axis=1)\n",
    "    ebsd = ebsd.reshape(100, 100, 3)\n",
    "    \n",
    "    om = np.array(load_img(path + 'om.png'))\n",
    "    om = tf.keras.utils.normalize(om, axis=1)\n",
    "    om = om.reshape(2000, 2000, 1)\n",
    "    \n",
    "    train_data.append([bse, ebsd, om, 918])\n",
    "\n",
    "# shuffle the data array\n",
    "\n",
    "random.shuffle(train_data)\n",
    "\n",
    "# load data into input data\n",
    "bse_train = []\n",
    "ebsd_train = []\n",
    "om_train = []\n",
    "stress_train = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    bse_train.append(train_data[i][0])\n",
    "    ebsd_train.append(train_data[i][1])\n",
    "    om_train.append(train_data[i][2])\n",
    "    stress_train.append(train_data[i][3])\n",
    "\n",
    "\n",
    "# reshape the arrays of each thing\n",
    "bse_train.reshape(20, 1_000, 1_000, 1)\n",
    "ebsd_train.reshape(20, 100, 100, 3)\n",
    "om_train.reshape(20, 2_000, 2_000, 1)\n",
    "stress_train.reshape(20, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2a965e668039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m           callbacks=[tensorboard_callback])\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Dutta_Research\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Dutta_Research\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Dutta_Research\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    495\u001b[0m                      'at same time.')\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m   \u001b[1;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Dutta_Research\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 653\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    654\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'numpy.ndarray\\'>\"})'}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})"
     ],
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'numpy.ndarray\\'>\"})'}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})",
     "output_type": "error"
    }
   ],
   "source": [
    "# FIT MODEL\n",
    "# callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,\n",
    "                                                      write_graph=True, write_images=True)\n",
    "\n",
    "model.fit([bse_train, ebsd_train, om_train],\n",
    "          [stress_train], \n",
    "          epochs = 1,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[tensorboard_callback])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}